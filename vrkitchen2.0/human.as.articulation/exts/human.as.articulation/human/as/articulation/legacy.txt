    /home/yizhou/.local/share/ov/pkg/isaac_sim-2022.1.0/jupyter_notebook.sh
    
    async def load_world_async(self):
        from omni.isaac.core.utils.stage import create_new_stage_async, update_stage_async
        from omni.isaac.core import World
        """Function called when clicking load buttton
        """
       
        if World.instance() is None:
            await create_new_stage_async()
            self._world = World(**self._world_settings)
            await self._world.initialize_simulation_context_async()
            # self.setup_scene()
        else:
            self._world = World.instance()
        self._current_tasks = self._world.get_current_tasks()
        await self._world.reset_async()
        await self._world.pause_async()
        await self.setup_post_load()
        if len(self._current_tasks) > 0:
            self._world.add_physics_callback("tasks_step", self._world.step_async)
        return

    def _on_physics_step(self, dt):
        if not self._can_callback_physics_step():
            return

        # call user implementation
        self.on_physics_step(dt)

    def on_physics_step(self, dt):
        """
        This method is called on each physics step callback, and the first callback is issued
        after the on_tensor_start method is called if the tensor API is enabled.
        """
        pass


# for i in range(5):
#     total_step += 1
#     # copy old obs
#     old_obs_buf = env.obs_buf.clone()
    
#     # step
#     if total_step < trainer.warm_up_steps or np.random.rand() < 1000 / total_step:
#         actions = 2 * torch.rand(env.num_envs, 21) - 1
#         actions = actions.to(env.device)
#     else:
#         actions = trainer.sample_action(old_obs_buf)
        
#     env.step(actions)
#     world.step(render=False)
    
#     # renew obs
#     env.compute_observations()
#     new_obs_buf = env.obs_buf.clone()
#     reward, done = env.compute_reward()
    
#     # stack memory
#     if torch.max(new_obs_buf) < 1e3:
#         # only record reasonble results
#         trainer.buf.add_batch(old_obs_buf, actions, new_obs_buf, reward, done)
    
#     if total_step % 100 == 0:
#         world.render()
#         # print(total_step, "obs peek", "reward: ", *reward.tolist(), "\n done: ", *done.tolist())
#         print("total_step:", total_step, torch.mean(env.reward_buf).item())
        
#     if total_step % 1000 < 30:
#         world.render()
    
#     # need to reset
#     if torch.sum(done) >= 1:
#         env_ids = []
#         for i in range(len(done)):
#             if done[i] > 0:
#                 env_ids.append(i)

#         env.reset_idx(env_ids)
        
#     # train
#     if trainer.buf.size > 1000:
#         # print("training sac")
#         trainer.train_debug()